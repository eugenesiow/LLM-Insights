# LLM Insights

1. Infrastructure/Platform
   - [Hardware](infrastructure/hardware)
2. Data Engineering
3. Pre-Training Stage
   - [Tokenization](pretraining/tokenization/README.md)
   - [Model Architecture](pretraining/architecture/README.md)
   - [Multi-Node](pretraining/multi-node/README.md)
   - [Training](pretraining/training/README.md)
4. Fine-Tuning Stage
   - [Adapters](finetuning/adapters/)
5. Alignment Stage
6. Inference Stage
   - [Estimating Model Size](inference/README.md##estimating-model-size)
   - [Speed](inference/README.md#generative-inference-speed)
   - [Throughput](inference/README.md#generative-inference-throughput)
7. Applications

## Acknowledgements

I'm grateful to my employers for trusting me to lead the team that built the GPU supercompute platform/infrastructure and to co-lead the team doing LLM pre-training. This allowed me to work on large on-premise GPU compute clusters with A100s and then H100s, which is certainly a privilege. Hopefully sharing some of these notes and insights helps the community.