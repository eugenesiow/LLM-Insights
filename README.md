# LLM Insights

1. Infrastructure/Platform
2. Data Engineering
3. [Pre-Training Stage](pretraining/)
   - [Tokenization](pretraining/tokenization/README.md)
     - [All Tokenizers Are Not Made Equal](pretraining/tokenization/README.md#all-tokenizers-are-not-made-equal)
   - [Model Architecture](pretraining/architecture/README.md)
   - [Multi-Node](pretraining/multi-node/README.md)
   - [Training](pretraining/training/README.md)
4. Fine-Tuning Stage
5. Alignment Stage
6. [Inference Stage](inference/)
   1. [Speed](inference/README.md#generative-inference-speed)
   2. [Throughput](inference/README.md#generative-inference-throughput)
7. Applications

## Acknowledgements

I'm grateful to my employers for trusting me to lead the team that built the GPU supercompute platform/infrastructure and co-lead the team doing LLM pre-training. This allowed me to work on huge on-premise GPU compute clusters with A100s and then H100s which is certainly a privilege. Hopefully sharing some of these notes and insights helps the community.