# Long-Context

Evaluations that involve answering questions on long documents (e.g. 32K, 64K, 128K, 1M), where the knowledge required from the document could be anywhere in the document.

- [MRCR (Multi-Round Coreference Resolution)](mrcr.md) - the model sees a long conversation between a user and a model, in which the user requests writing (e.g. poems, riddles, essays) on different topics proceeded by the model responses. In each conversation, two user requests containing topics and writing formats distinct from the rest of the conversation are randomly placed in the context. Given the conversation, the model must reproduce the model's output (the needle) resulting from one of the two requests (the key).
- [ZeroSCROLLS](zeroscrolls.md) - suite of datasets that require synthesizing information over **long texts**. The benchmark includes ten natural language tasks across multiple domains, including summarization, question answering, aggregated sentiment classification and information reordering.
- [InfiniteBench](infinitebench.md) - samples with an average data length surpassing 100K tokens. Tasks are designed to require the understanding of long dependencies in contexts and are harder than simply retrieving a limited number of passages from contexts.
- [NIH (Needle In a Haystack)](nih.md) - analysis to test in-context retrieval ability of long-context LLMs. By placing a random fact or statement (the 'needle') in the middle of a long context window (the 'haystack') and testing the models ability to retrieve this statement.
- [LongBench](longbench.md) - benchmark designed to evaluate the capability of large language models (LLMs) to perform deep understanding and reasoning tasks over exceptionally long contexts, ranging from 8K to 2M n words.
- [RULER](ruler.md) - synthetic benchmark designed for evaluation of long-context language models, featuring flexible configurations for customized sequence length and task complexity and testing behaviors beyond simple retrieval.