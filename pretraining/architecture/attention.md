# Attention

## References
1. [GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints](https://arxiv.org/abs/2305.13245)
2. [Understanding Multi-Head Latent Attention](https://planetbanatt.net/articles/mla.html)