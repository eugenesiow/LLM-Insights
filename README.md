# LLM Insights

1. Infrastructure/Platform
   - [Hardware](infrastructure/hardware/)
   - [Alternative Accelerators](infrastructure/hardware/alternatives/)
2. Data Engineering
   - [Common Crawl](dataengineering/cc/README.md)
3. Pre-Training Stage
   - [Tokenization](pretraining/tokenization/)
   - [Model Architecture](pretraining/architecture/)
   - [Multi-Node](pretraining/multi-node/)
   - [Training](pretraining/training/)
   - [Base Model Evaluation](pretraining/evaluation/)
4. Fine-Tuning Stage
   - [Adapters](finetuning/adapters/)
5. Alignment Stage
   - [Reducing Hallucination](alignment/hallucination/)
6. Evaluation Stage
   - [Reasoning](evaluation/reasoning/README.md)
7. Inference Stage
   - [Estimating Model Size](inference/README.md##estimating-model-size)
   - [Speed](inference/README.md#generative-inference-speed)
   - [Throughput](inference/README.md#generative-inference-throughput)
   - [Guidance](inference/guidance/README.md)
8. Applications
   - [Agents](applications/agents/README.md)

## Acknowledgements

I'm grateful to my employers for trusting me to lead the team that built the GPU supercompute platform/infrastructure and to co-lead the team doing LLM pre-training. This allowed me to work on large on-premise GPU compute clusters with A100s and then H100s, which is certainly a privilege. Hopefully sharing some of these notes and insights helps the community.