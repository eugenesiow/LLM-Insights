# Long-Context

Evaluations that involve answering questions on long documents (e.g. 32K, 64K, 128K, 1M), where the knowledge required from the document could be anywhere in the document.

- [MRCR (Multi-Round Coreference Resolution)](mrcr.md) - the model sees a long conversation between a user and a model, in which the user requests writing (e.g. poems, riddles, essays) on different topics proceeded by the model responses. In each conversation, two user requests containing topics and writing formats distinct from the rest of the conversation are randomly placed in the context. Given the conversation, the model must reproduce the model's output (the needle) resulting from one of the two requests (the key).
- [ZeroSCROLLS](zeroscrolls.md) - suite of datasets that require synthesizing information over **long texts**. The benchmark includes ten natural language tasks across multiple domains, including summarization, question answering, aggregated sentiment classification and information reordering.
- [InfiniteBench](infinitebench.md) - samples with an average data length surpassing 100K tokens. Tasks are designed to require the understanding of long dependencies in contexts and are harder than simply retrieving a limited number of passages from contexts.