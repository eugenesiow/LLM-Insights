# Base Model Evaluation

- [Base Model Evaluation](#base-model-evaluation)
  - [Performance Tracking](#performance-tracking)
  - [Small Evaluations](#small-evaluations)
  - [Problem Solving](#problem-solving)
    - [MMLU](#mmlu)
    - [BBH](#bbh)
    - [HumanEval](#humaneval)
    - [DROP](#drop)
  - [Commonsense Reasoning Tasks](#commonsense-reasoning-tasks)
    - [HellaSwag](#hellaswag)
    - [OBQA](#obqa)
    - [WinoGrande](#winogrande)
    - [ARC](#arc)
    - [BoolQ](#boolq)
    - [PIQA](#piqa)

See the [evaluation section](../evaluation/) for more details.

## Performance Tracking

| Model                                          | Model Family          | Model Size (B) | Pretraining Data Size (T) | FLOPs (1E21) | MMLU         | ARC-C        | HellaSwag    | Winograd     | TruthfulQA   | GSM8K         | XWinograd    | HumanEval     |
|------------------------------------------------|-----------------------|----------------|---------------------------|--------------|--------------|--------------|--------------|--------------|--------------|---------------|--------------|---------------|
| meta-llama/Llama-2-7b-hf                       | Llama-2               | 7              | 2                         | 84           | 0.4379609636 | 0.5307167235 | 0.7774347739 | 0.7403314917 | 0.389802028  | 0.1448066717  | 0.7548812733 | 0.1280487805  |
| meta-llama/Llama-2-13b-hf                      | Llama-2               | 13             | 2                         | 156          | 0.543378     | 0.5810580205 | 0.809699     | 0.7663772691 | 0.3417240296 | 0.2282031842  | 0.786798487  | 0.1829268293  |
| meta-llama/Llama-2-70b-hf                      | Llama-2               | 70             | 2                         | 840          | 0.6983208921 | 0.6732081911 | 0.8733320056 | 0.8374112076 | 0.4492349372 | 0.5405610311  | 0.8244532276 | 0.29878       |
| huggyllama/llama-7b                            | Llama                 | 6.7            | 1                         | 40.2         | 0.3569330639 | 0.5093856655 | 0.7781318462 | 0.7142857143 | 0.3432793294 | 0.080364      | 0.693204     | 0.1280487805  |
| huggyllama/llama-13b                           | Llama                 | 13             | 1                         | 78           | 0.476105883  | 0.5614334471 | 0.8092013543 | 0.7624309392 | 0.3947888264 | 0.075815      | 0.7303616814 | 0.1585365854  |
| huggyllama/llama-30b                           | Llama                 | 32.5           | 1.4                       | 273          | 0.5844661284 | 0.614334471  | 0.8473411671 | 0.8003157064 | 0.42273876   | 0.1485974223  | 0.7711479042 | 0.2073170732  |
| huggyllama/llama-65b                           | Llama                 | 65.2           | 1.4                       | 547.68       | 0.6393064176 | 0.6348122867 | 0.860884286  | 0.8255722178 | 0.4342876071 | 0.3722517058  | 0.7768348072 | 0.2317073171  |
| meta-llama/Meta-Llama-3-70B                    | Llama-3               | 70             | 15                        | 6300         | 0.792329     |              | 0.8798048198 | 0.8531965272 | 0.455623682  | 0.7687642153  | 0.844706094  | 0.52439       |
| meta-llama/Meta-Llama-3-8B                     | Llama-3               | 8              | 15                        | 720          | 0.6649495493 |              | 0.8201553475 | 0.771113     | 0.4395226511 | 0.453373768   | 0.8011516647 | 0.3841463415  |
| meta-llama/Meta-Llama-3.1-405B-FP8             | Llama-3.1             | 405            | 15                        | 36450        | 0.8487448467 |              | 0.9005178251 | 0.861878     | 0.474126     | 0.8574677786  | 0.8549700687 | 0.5914634146  |
| meta-llama/Meta-Llama-3.1-70B                  | Llama-3.1             | 70             | 15                        | 6300         | 0.7876492387 | 0.6979522184 | 0.8803027285 | 0.8516179953 | 0.496705038  | 0.7626990144  | 0.8421425068 | 0.5975609756  |
| meta-llama/Meta-Llama-3.1-8B                   | Llama-3.1             | 8              | 15                        | 720          | 0.659864671  | 0.5767918089 | 0.8167695678 | 0.7837411208 | 0.452236648  | 0.4579226687  | 0.799428052  | 0.3719512195  |
| Qwen/Qwen1.5-110B                              | Qwen1.5               | 110            |                           |              | 0.8019575909 | 0.6996587031 | 0.8748257319 | 0.8413575375 | 0.4966492716 | 0.8104624716  | 0.827524735  | 0.5609756098  |
| Qwen/Qwen1.5-72B                               | Qwen1.5               | 72             | 3                         | 1296         | 0.7720145831 | 0.658703     | 0.8598884684 | 0.8303078137 | 0.596081     | 0.6573161486  | 0.8257680946 | 0.4512195122  |
| Qwen/Qwen1.5-32B                               | Qwen1.5               | 32             | 4                         | 768          | 0.7429961284 | 0.635665529  | 0.8500298745 | 0.8145224941 | 0.5739444339 | 0.6110689917  | 0.7911760589 | 0.4207317073  |
| Qwen/Qwen1.5-14B                               | Qwen1.5               | 14             | 4                         | 336          | 0.6936026471 | 0.5656996587 | 0.810795     | 0.7348066298 | 0.5206092395 | 0.6762699014  | 0.777502468  | 0.3963414634  |
| Qwen/Qwen1.5-7B                                | Qwen1.5               | 7              | 4                         | 168          | 0.6197002168 | 0.5418088737 | 0.7851025692 | 0.712707     | 0.5108227702 | 0.535254      | 0.7523613984 | 0.3475609756  |
| Qwen/Qwen1.5-4B                                | Qwen1.5               | 4              | 2.4                       | 57.6         | 0.5652351964 | 0.4846416382 | 0.7157936666 | 0.6621941594 | 0.4727318979 | 0.5223654284  | 0.6887854394 | 0.262195122   |
| Qwen/Qwen1.5-1.8B                              | Qwen1.5               | 1.8            | 2.4                       | 25.92        | 0.467083853  | 0.37884      | 0.6142202748 | 0.6029992107 | 0.3942781425 | 0.335861      | 0.643825     | 0.1829268293  |
| Qwen/Qwen1.5-0.5B                              | Qwen1.5               | 0.5            | 2.4                       | 7.2          | 0.3935334081 | 0.3148464164 | 0.4905397331 | 0.5722178374 | 0.3829926844 | 0.1630022745  | 0.5756214928 | 0.1158536585  |
| Qwen/Qwen-72B                                  | Qwen                  | 72             | 3                         | 1296         | 0.773737575  | 0.6518771331 | 0.8593905596 | 0.824783     | 0.6019109517 | 0.7043214556  | 0.8287314131 | 0.3719512195  |
| Qwen/Qwen-14B                                  | Qwen                  | 14             | 3                         | 252          | 0.6770428032 | 0.5827645051 | 0.8398725354 | 0.7679558011 | 0.4943294461 | 0.5898407885  | 0.791508524  | 0.3536585366  |
| Qwen/Qwen-7B                                   | Qwen                  | 7              | 2.4                       | 100.8        | 0.5983879208 | 0.5136518771 | 0.7847042422 | 0.72691397   | 0.4778593055 | 0.4495830174  | 0.7345767284 | 0.3170731707  |
| Qwen/Qwen2-72B                                 | Qwen2                 | 72             |                           |              | 0.8389158034 | 0.6877133106 | 0.8732324238 | 0.8421468035 | 0.547089672  | 0.8521607278  | 0.836833     | 0.6097560976  |
| Qwen/Qwen2-57B-A14B                            | Qwen2                 | 24             |                           |              | 0.7631831318 | 0.6416382253 | 0.8525194184 | 0.7900552486 | 0.5753944052 | 0.7657316149  | 0.8121267207 | 0.5182926829  |
| Qwen/Qwen2-7B                                  | Qwen2                 | 7              |                           |              | 0.719491906  | 0.6109215017 | 0.8063134834 | 0.7624309392 | 0.5434989797 | 0.7445034117  | 0.7820493215 | 0.4695121951  |
| Qwen/Qwen2-1.5B                                | Qwen2                 | 1.5            |                           |              | 0.5719800135 | 0.4385665529 | 0.6671977694 | 0.6464088398 | 0.4588177707 | 0.5564821835  | 0.7323140692 | 0.3353658537  |
| Qwen/Qwen2-0.5B                                | Qwen2                 | 0.5            |                           |              | 0.4516942942 | 0.319112628  | 0.4869547899 | 0.5595895817 | 0.3969971151 | 0.3457164519  | 0.627435293  | 0.2317073171  |
| mistralai/Mistral-7B-v0.1                      | Mistral               | 7.3            |                           |              | 0.6416448378 | 0.5998293515 | 0.8331009759 | 0.7861089187 | 0.4215317107 | 0.3707354056  | 0.7818571022 | 0.27439       |
| mistralai/Mixtral-8x7B-v0.1                    | Mixtral               | 45             |                           |              | 0.7187839332 | 0.6638225256 | 0.864568811  | 0.816890292  | 0.46805433   | 0.576194      | 0.800190967  | 0.3353658537  |
| mistralai/Mixtral-8x22B-v0.1                   | Mixtral               | 39             |                           |              | 0.7779499653 | 0.7064846416 | 0.8873730333 | 0.8500394633 | 0.50951604   | 0.7369219105  | 0.8401819899 | 0.4207317073  |
| mistralai/Mistral-Nemo-Base-2407               | Mistral               | 12             |                           |              | 0.6966823126 | 0.6390784983 | 0.8526190002 | 0.817679558  | 0.490645983  | 0.5276724792  | 0.8181873269 | 0.3658536585  |
| deepseek-ai/DeepSeek-V2                        | DeepSeek-V2           | 21             | 8.1                       | 1020.6       | 0.7886879734 | 0.6860068259 | 0.8778131846 | 0.8382004736 | 0.4148269028 | 0.7672479151  | 0.8316106037 | 0.4695121951  |
| deepseek-ai/DeepSeek-Coder-V2-Base             | DeepSeek-Coder-V2     | 21             | 10.2                      | 1285.2       | 0.798055495  | 0.681741     | 0.860485959  | 0.8342541436 | 0.4144856187 | 0.8021228203  | 0.8284172546 | 0.5304878049  |
| 01-ai/Yi-6B                                    | Yi                    | 6              | 3                         | 108          | 0.641070914  | 0.5554607509 | 0.7656841267 | 0.7419100237 | 0.419616075  | 0.1213040182  | 0.7238791259 | 0.1585365854  |
| 01-ai/Yi-34B                                   | Yi                    | 34             | 3                         | 612          | 0.7635156727 | 0.6459044369 | 0.8569010157 | 0.8303078137 | 0.5623083933 | 0.506444276   | 0.7956404906 | 0.2682926829  |
| 01-ai/Yi-1.5-34B                               | Yi-1.5                | 34             | 3.6                       | 734.4        | 0.7806872178 | 0.6578498294 | 0.8611830313 | 0.8358326756 | 0.5388377437 | 0.7384382108  | 0.7774272632 | 0.4573170732  |
| 01-ai/Yi-1.5-9B                                | Yi-1.5                | 9              | 3.6                       | 194.4        | 0.7099770832 | 0.6194539249 | 0.8031268672 | 0.7797947908 | 0.466745566  | 0.6277482942  | 0.7419586924 | 0.4207317073  |
| 01-ai/Yi-1.5-6B                                | Yi-1.5                | 6              | 3.6                       | 129.6        | 0.649091455  | 0.5716723549 | 0.7797251544 | 0.7529597474 | 0.4401230806 | 0.4981046247  | 0.7137746099 | 0.3658536585  |
| google/gemma-7b                                | Gemma                 | 7              | 6                         | 252          | 0.6602745725 | 0.6109215017 | 0.8247361083 | 0.7845303867 | 0.449054884  | 0.5276724792  | 0.783899856  | 0.3353658537  |
| google/gemma-2b                                | Gemma                 | 2              | 6                         | 72           | 0.4177314644 | 0.4837883959 | 0.71768572   | 0.6629834254 | 0.3308443428 | 0.1690674754  | 0.7093183574 | 0.2317073171  |
| google/gemma-2-27b                             | Gemma-2               | 27             | 13                        | 2106         | 0.752        | 0.714        | 0.8703445529 | 0.873        | 0.477656     | 0.74          | 0.8284573297 | 0.5304878049  |
| google/gemma-2-9b                              | Gemma-2               | 9              | 8                         | 432          | 0.713        | 0.684        | 0.819        | 0.806        | 0.4643743533 | 0.686         | 0.8301511187 | 0.3963414634  |
| google/gemma-2-2b                              | Gemma-2               | 2              | 2                         | 24           | 0.522        | 0.557        | 0.729        | 0.713        | 0.4133173558 | 0.243         | 0.7589869855 | 0.1951219512  |
| ai21labs/Jamba-v0.1                            | Jamba                 | 12             |                           |              | 0.5813383693 | 0.5938566553 | 0.8282214698 | 0.7411207577 | 0.4535262905 | 0.2137983321  | 0.7252442868 | 0.3170731707  |
| tiiuae/falcon-180B                             | Falcon                | 180            | 3.5                       | 3780         | 0.6958661043 | 0.6919795222 | 0.8888667596 | 0.8689818469 | 0.451569509  | 0.4594389689  | 0.8445533519 |               |
| tiiuae/falcon-40b                              | Falcon                | 40             | 1                         | 240          | 0.569797005  | 0.6194539249 | 0.8528181637 | 0.8129439621 | 0.4171692245 | 0.2145564822  | 0.784630576  |               |
| tiiuae/falcon-7b                               | Falcon                | 7              | 1.5                       | 63           | 0.2778547015 | 0.478668942  | 0.7813184625 | 0.7237569061 | 0.342638     | 0.046247      | 0.7175723632 |               |
| tiiuae/falcon-rw-1b                            | Falcon                | 1              | 0.35                      | 2.1          | 0.2528031885 | 0.3506825939 | 0.6356303525 | 0.6203630624 | 0.3595559898 | 0.005307      | 0.5354987006 |               |
| microsoft/phi-2                                | Phi                   | 2.7            | 1.4                       | 22.68        | 0.5792026667 | 0.6100682594 | 0.7491535551 | 0.7348066298 | 0.4423687837 | 0.5496588324  | 0.5266663911 | 0.493902439   |
| microsoft/phi-1_5                              | Phi                   | 1.3            | 0.15                      | 1.17         | 0.4388646778 | 0.5290102389 | 0.6379207329 | 0.7221783741 | 0.4088993856 | 0.1243366187  | 0.5111127838 | 0.3414634146  |
| EleutherAI/pythia-1b-deduped                   | Pythia                | 1              | 0.3                       | 1.8          | 0.2427106456 | 0.2909556314 | 0.4965146385 | 0.5359116022 | 0.389394     | 0.011372      | 0.560992     | 0.042683      |
| EleutherAI/pythia-410m-deduped                 | Pythia                | 0.41           | 0.3                       | 0.738        | 0.2598881956 | 0.2482935154 | 0.412865963  | 0.543804262  | 0.4094757859 | 0.003033      | 0.5363121646 | 0.012195      |
| EleutherAI/pythia-6.9b-deduped                 | Pythia                | 6.9            | 0.3                       | 12.42        | 0.2648094407 | 0.4129692833 | 0.6704839673 | 0.6408839779 | 0.3519458488 | 0.016679      | 0.6524704541 | 0.085366      |
| EleutherAI/pythia-2.8b-deduped                 | Pythia                | 2.8            | 0.3                       | 5.04         | 0.2678463743 | 0.3626279863 | 0.6065524796 | 0.6022099448 | 0.3555978235 | 0.00834       | 0.6399890469 | 0.04878       |
| EleutherAI/pythia-12b-deduped                  | Pythia                | 12             | 0.3                       | 21.6         | 0.256306029  | 0.4138225256 | 0.7026488747 | 0.664562     | 0.329989     | 0.014405      | 0.6824205147 | 0.1158536585  |
| EleutherAI/pythia-70m-deduped                  | Pythia                | 0.07           | 0.3                       | 0.126        | 0.2526050704 | 0.2107508532 | 0.2716590321 | 0.4964483031 | 0.4751438548 | 0             | 0.5100999142 | 0             |
| EleutherAI/pythia-1.4b-deduped                 | Pythia                | 1.4            | 0.3                       | 2.52         | 0.2555549665 | 0.3267918089 | 0.5495917148 | 0.5730071034 | 0.3865846445 | 0.00834       | 0.5940628537 | 0.042683      |
| EleutherAI/pythia-160m-deduped                 | Pythia                | 0.16           | 0.3                       | 0.288        | 0.248606     | 0.2406143345 | 0.3138816969 | 0.5138121547 | 0.443404     | 0.002274      | 0.5235998028 | 0             |
| bigscience/bloom-560m                          | BLOOM                 | 0.56           | 0.341                     | 1.14576      | 0.2421541212 | 0.247440273  | 0.37154      | 0.5193370166 | 0.4244428174 | 0.003033      | 0.5785515149 | 0.006098      |
| bigscience/bloom-1b1                           | BLOOM                 | 1.1            | 0.341                     | 2.2506       | 0.2670214826 | 0.2832764505 | 0.4278032264 | 0.5501183899 | 0.4179716704 | 0.002274      | 0.6095129616 | 0             |
| bigscience/bloom-3b                            | BLOOM                 | 3              | 0.341                     | 6.138        | 0.265925092  | 0.3575085324 | 0.543716     | 0.5761641673 | 0.4057246299 | 0.015163      | 0.664767     | 0.018293      |
| bigscience/bloom-7b1                           | BLOOM                 | 7.1            | 0.341                     | 14.5266      | 0.262462     | 0.4112627986 | 0.6199960167 | 0.654301     | 0.3889784219 | 0.013647      | 0.697719     | 0.04878       |
| bigscience/bloom                               | BLOOM                 | 176            | 0.366                     | 386.496      | 0.3085400906 | 0.5042662116 | 0.7640908186 | 0.7205998421 | 0.3975962282 | 0.068992      | 0.735544     | 0.1219512195  |
| EleutherAI/gpt-neox-20b                        | GPT-Neo/J             | 20             | 0.472                     | 56.64        | 0.250025953  | 0.457337884  | 0.734515     | 0.6890292028 | 0.3161314597 | 0.054587      | 0.7162809752 | 0.1280487805  |
| EleutherAI/gpt-neo-2.7B                        | GPT-Neo/J             | 2.7            | 0.42                      | 6.804        | 0.2645397884 | 0.333618     | 0.5624377614 | 0.6006314128 | 0.3977795991 | 0.012889      | 0.573989     | 0.067073      |
| EleutherAI/gpt-neo-1.3B                        | GPT-Neo/J             | 1.3            | 0.38                      | 2.964        | 0.248216555  | 0.3122866894 | 0.4846644095 | 0.569061     | 0.3962930604 | 0.004549      | 0.5610910983 | 0.036585      |
| EleutherAI/gpt-neo-125m                        | GPT-Neo/J             | 0.125          | 0.3                       | 0.225        | 0.2597039138 | 0.2295221843 | 0.3026289584 | 0.5177584846 | 0.455762     | 0.003033      | 0.5022135307 | 0.006098      |
| EleutherAI/gpt-j-6b                            | GPT-Neo/J             | 6.05           | 0.402                     | 14.5926      | 0.2678399936 | 0.4138225256 | 0.675363     | 0.659826     | 0.359625     | 0.029568      | 0.681077656  | 0.1158536585  |
| facebook/opt-6.7b                              | OPT                   | 6.7            | 0.18                      | 7.236        | 0.245674066  | 0.3916382253 | 0.6866162119 | 0.659826     | 0.3512214979 | 0.009856      | 0.5942690811 | 0.006098      |
| facebook/opt-1.3b                              | OPT                   | 1.3            | 0.18                      | 1.404        | 0.2496304583 | 0.295222     | 0.54531      | 0.5974743489 | 0.387107     | 0.001516      | 0.5440287611 | 0             |
| facebook/opt-350m                              | OPT                   | 0.35           | 0.18                      | 0.378        | 0.2602309674 | 0.2354948805 | 0.367258     | 0.52644      | 0.4082853278 | 0.003033      | 0.5181109402 | 0             |
| facebook/opt-13b                               | OPT                   | 13             | 0.18                      | 14.04        | 0.2489566224 | 0.3993174061 | 0.7120095598 | 0.6850828729 | 0.3409583991 | 0.017437      | 0.6087793556 | 0.006098      |
| facebook/opt-2.7b                              | OPT                   | 2.7            | 0.18                      | 2.916        | 0.2543197295 | 0.3395904437 | 0.6143198566 | 0.6195737964 | 0.3742541799 | 0.002274      | 0.5684868941 | 0             |
| facebook/opt-30b                               | OPT                   | 30             | 0.18                      | 32.4         | 0.2665873004 | 0.4325938567 | 0.7406891058 | 0.7063930545 | 0.351638     | 0.021986      | 0.626407888  | 0.012195      |
| facebook/opt-125m                              | OPT                   | 0.125          | 0.18                      | 0.135        | 0.2601558756 | 0.228668942  | 0.3146783509 | 0.51618      | 0.4286875803 | 0.000758      | 0.4986842384 | 0             |
| facebook/opt-66b                               | OPT                   | 66             | 0.18                      | 71.28        | 0.2699300206 | 0.4633105802 | 0.7624975105 | 0.7000789266 | 0.3542965595 | 0.016679      | 0.6426314629 | 0.012195      |
| mosaicml/mpt-30b                               | MPT                   | 30             | 1                         | 180          | 0.4799818067 | 0.5597269625 | 0.8242381996 | 0.7490134175 | 0.3841558252 | 0.1690674754  | 0.745335406  | 0.2134146341  |
| mosaicml/mpt-7b                                | MPT                   | 7              | 1                         | 42           | 0.2806843136 | 0.4769624573 | 0.7753435571 | 0.7213891081 | 0.3354506044 | 0.040182      | 0.7144       | 0.1646341463  |
| facebook/xglm-564M                             | XGLM                  | 0.564          | 0.5                       | 1.692        | 0.2517966357 | 0.2457337884 | 0.3464449313 | 0.5224940805 | 0.4043205877 | 0.002274      | 0.5855090555 | 0             |
| facebook/xglm-1.7B                             | XGLM                  | 1.7            | 0.5                       | 5.1          | 0.2510041646 | 0.2585324232 | 0.4567815176 | 0.5390686661 | 0.3720634152 | 0.007582      | 0.6306808718 | 0             |
| facebook/xglm-4.5B                             | XGLM                  | 4.5            | 0.5                       | 13.5         | 0.2543       | 0.3148464164 | 0.5794662418 | 0.5493291239 | 0.3583939485 | 0.002274      | 0.6585139569 | 0             |
| facebook/xglm-7.5B                             | XGLM                  | 7.5            | 0.5                       | 22.5         | 0.2778866807 | 0.3412969283 | 0.6077474607 | 0.5872138911 | 0.3666152388 | 0.002274      | 0.6955505613 | 0             |
| codellama/CodeLlama-7b-hf                      | CodeLlama             | 7              | 2.52                      | 105.84       | 0.3112206483 | 0.3993174061 | 0.6080462059 | 0.6400947119 | 0.3782167557 | 0.051554      | 0.72972396   | 0.3353658537  |
| codellama/CodeLlama-13b-hf                     | CodeLlama             | 13             | 2.52                      | 196.56       | 0.3280883582 | 0.4087030717 | 0.6335391356 | 0.6716653512 | 0.4379426978 | 0.1213040182  | 0.7349485726 | 0.3841463415  |
| codellama/CodeLlama-34b-hf                     | CodeLlama             | 34             | 2.52                      | 514.08       | 0.5502083994 | 0.5409556314 | 0.7582154949 | 0.7355958958 | 0.3911361839 | 0.3434420015  | 0.7861245882 | 0.4756097561  |
| codellama/CodeLlama-70b-hf                     | CodeLlama             | 70             | 3.02                      | 1268.4       | 0.5966658993 | 0.5674061433 | 0.7821151165 | 0.7521704815 | 0.3978847741 | 0.439727066   | 0.7756388556 | 0.5487804878  |
| bigcode/starcoderbase-1b                       | StarCoder             | 1              | 1                         | 6            | 0.266739     | 0.2269624573 | 0.3430591516 | 0.499605367  | 0.4578928665 | 0.009098      | 0.561739     | 0.146         |
| bigcode/starcoderbase-3b                       | StarCoder             | 3              | 1                         | 18           | 0.2735280049 | 0.2585324232 | 0.3910575583 | 0.5114443567 | 0.4305451199 | 0.017437      | 0.5976355731 | 0.177         |
| bigcode/starcoderbase-7b                       | StarCoder             | 7              | 1                         | 42           | 0.2844643845 | 0.2986348123 | 0.4386576379 | 0.543804262  | 0.4046263361 | 0.054587      | 0.5978311245 | 0.244         |
| bigcode/starcoderbase                          | StarCoder             | 15.5           | 1                         | 93           | 0.3211709749 | 0.302901     | 0.4721171081 | 0.5580110497 | 0.400216     | 0.078848      | 0.5951901705 | 0.341         |
| bigcode/starcoder2-15b                         | StarCoder2            | 15             | 4.3                       | 387          | 0.5134583866 | 0.4735494881 | 0.6409081856 | 0.63851618   | 0.378734     | 0.5223654284  | 0.7382820133 | 0.463         |
| bigcode/starcoder2-7b                          | StarCoder2            | 7              | 3.7                       | 155.4        | 0.4121331864 | 0.383105802  | 0.5191196973 | 0.591949487  | 0.4199349766 | 0.2509476876  | 0.6200898067 | 0.354         |
| bigcode/starcoder2-3b                          | StarCoder2            | 3              | 3.3                       | 59.4         | 0.3864860207 | 0.3455631399 | 0.4761999602 | 0.545382794  | 0.4048698237 | 0.1963608795  | 0.603683     | 0.317         |
| deepseek-ai/deepseek-coder-1.3b-base           | DeepSeek-Coder        | 1.3            | 2                         | 15.6         | 0.2602473162 | 0.257679     | 0.3927504481 | 0.5272296764 | 0.4261368206 | 0.029568      | 0.6063458156 | 0.287         |
| deepseek-ai/deepseek-coder-6.7b-base           | DeepSeek-Coder        | 6.7            | 2                         | 80.4         | 0.3839143098 | 0.3703071672 | 0.5345548695 | 0.5808997632 | 0.402813     | 0.179681577   | 0.6788708195 | 0.476         |
| deepseek-ai/deepseek-coder-33b-base            | DeepSeek-Coder        | 33             | 2                         | 396          | 0.4091373085 | 0.4249146758 | 0.5998805019 | 0.6243093923 | 0.3997018824 | 0.300227445   | 0.696132     | 0.512         |
| 01-ai/Yi-6B-200K                               | Yi-200K               | 6              | 3                         | 108          | 0.6464930135 | 0.5358361775 | 0.755826     | 0.7426992897 | 0.4174259548 | 0.3032600455  |              |               |
| 01-ai/Yi-34B-200K                              | Yi-200K               | 34             | 3                         | 612          | 0.7556180045 | 0.6578498294 | 0.8205536746 | 0.8287292818 | 0.4260214588 | 0.3487490523  |              |               |
| openlm-research/open_llama_3b_v2               | OpenLlamaV2           | 3              | 1                         | 18           | 0.2711798101 | 0.4027303754 | 0.7159928301 | 0.670087     | 0.3477745325 | 0.009098      | 0.6629864334 | 0.030488      |
| openlm-research/open_llama_7b_v2               | OpenLlamaV2           | 13             | 1                         | 78           | 0.4129397386 | 0.4368600683 | 0.7219677355 | 0.6937647987 | 0.3553921543 | 0.03487490523 | 0.7061066098 | 0.060976      |
| openlm-research/open_llama_7b                  | OpenLlama             | 7              | 1                         | 42           | 0.3049340815 | 0.4701365188 | 0.7197769369 | 0.679558011  | 0.3484730707 | 0.015921      | 0.6784067802 | 0             |
| openlm-research/open_llama_3b                  | OpenLlama             | 3              | 1                         | 18           | 0.2693628704 | 0.3984641638 | 0.6264688309 | 0.6471981058 | 0.3496513582 | 0.004549      | 0.6303847286 | 0             |
| openlm-research/open_llama_13b                 | OpenLlama             | 13             | 0.6                       | 46.8         | 0.4375452834 | 0.5119453925 | 0.7523401713 | 0.7205998421 | 0.380759673  | 0.03260045489 |              |               |
| openai-community/gpt2                          | GPT-2                 | 0.124          |                           |              | 0.258337178  | 0.2201365188 | 0.3152758415 | 0.5043409629 | 0.40691164   | 0.006823      | 0.4836287229 | 0             |
| openai-community/gpt2-medium                   | GPT-2                 | 0.355          |                           |              | 0.2682941554 | 0.2721843003 | 0.401912     | 0.5398579321 | 0.4073275935 | 0.005307      | 0.496119     | 0             |
| openai-community/gpt2-large                    | GPT-2                 | 0.774          |                           |              | 0.2606740306 | 0.257679     | 0.4561840271 | 0.5540647198 | 0.3871845204 | 0.00834       | 0.485048314  | 0             |
| openai-community/gpt2-xl                       | GPT-2                 | 1.5            |                           |              | 0.2654916591 | 0.302901     | 0.5139414459 | 0.58011      | 0.3853406812 | 0.009098      | 0.5318969762 | 0             |
| internlm/internlm2-20b                         | InternLM2             | 20             |                           |              | 0.6758066863 | 0.6296928328 | 0.832105     | 0.8555643252 | 0.5126520041 | 0.6793025019  |              |               |
| internlm/internlm2-7b                          | InternLM2             | 7              |                           |              | 0.6524284759 | 0.5802047782 | 0.8123879705 | 0.8382004736 | 0.4872683929 | 0.6300227445  | 0.7590335542 |               |
| deepseek-ai/deepseek-llm-67b-base              | DeepSeek-LLM          | 67             | 2                         | 804          | 0.7177932742 | 0.6544368601 | 0.8710416252 | 0.8413575375 | 0.5108013665 | 0.5670962851  |              |               |
| deepseek-ai/deepseek-llm-7b-base               | DeepSeek-LLM          | 7              | 2                         | 84           | 0.4924382664 | 0.5170648464 | 0.7811192989 | 0.7434885556 | 0.3492387119 | 0.1645185747  | 0.7538762024 | 0.262195122   |
| deepseek-ai/deepseek-moe-16b-base              | DeepSeek-MoE          | 16             | 2                         | 192          | 0.4630611356 | 0.5324232082 | 0.7977494523 | 0.7371744278 | 0.3607930335 | 0.1728582259  |              |               |
| Deci/DeciLM-7B                                 | DeciLM                | 7              |                           |              | 0.5976293892 | 0.5938566553 | 0.8251344354 | 0.7995264404 | 0.4032625331 | 0.4738438211  |              |               |
| stabilityai/stablelm-base-alpha-3b             | StableLM              | 3              | 1.5                       | 27           | 0.254305266  | 0.2645051195 | 0.4224258116 | 0.5390686661 | 0.4049657855 | 0.004549      | 0.533158971  | 0.030488      |
| stabilityai/stablelm-base-alpha-7b-v2          | StableLM              | 7              | 1.1                       | 46.2         | 0.4509893993 | 0.4735494881 | 0.770763     | 0.6850828729 | 0.3645720175 | 0.025777      | 0.693316     | 0.1463414634  |
| stabilityai/stablelm-2-1_6b                    | StableLM              | 1.6            | 2                         | 19.2         | 0.389465726  | 0.433447099  | 0.7045409281 | 0.6456195738 | 0.3678385824 | 0.1743745262  | 0.6893882983 | 0.1036585366  |
| stabilityai/stablelm-base-alpha-7b             | StableLM              | 7              | 1.5                       | 63           | 0.2620516095 | 0.3199658703 | 0.5178251344 | 0.5540647198 | 0.4019376597 | 0.006065      | 0.5622132967 | 0.0243902439  |
| RWKV/rwkv-4-169m-pile                          | RWKV                  | 0.169          |                           |              | 0.2317624001 | 0.2363481229 | 0.3173670584 | 0.5090765588 | 0.4192323633 | 0.004549      |              |               |
| RWKV/rwkv-4-430m-pile                          | RWKV                  | 0.43           |                           |              | 0.2485496978 | 0.2670648464 | 0.4001194981 | 0.5114443567 | 0.3958221882 | 0.003791      |              |               |
| RWKV/rwkv-4-1b5-pile                           | RWKV                  | 1.5            |                           |              | 0.2577446961 | 0.318259     | 0.522505477  | 0.5382794002 | 0.357956961  | 0             |              |               |
| RWKV/rwkv-4-7b-pile                            | RWKV                  | 7              |                           |              | 0.2495936335 | 0.3967576792 | 0.663115     | 0.6235201263 | 0.3365298312 | 0.007582      |              |               |
| RWKV/rwkv-4-14b-pile                           | RWKV                  | 14             |                           |              | 0.261244     | 0.4445392491 | 0.710714997  | 0.654301     | 0.3204219399 | 0.003791      |              |               |
| RWKV/rwkv-raven-14b                            | RWKV                  | 14             |                           |              | 0.2591933099 | 0.4462457338 | 0.7125074686 | 0.6669297553 | 0.4193256932 | 0.021228      |              |               |
| RWKV/rwkv-4-3b-pile                            | RWKV                  | 3              |                           |              | 0.2467087225 | 0.3600682594 | 0.5965943039 | 0.5832675612 | 0.3213702263 | 0.006823      |              |               |
| togethercomputer/RedPajama-INCITE-Base-7B-v0.1 | RedPajama-INCITE-Base | 7              | 1                         | 42           | 0.2768214019 | 0.4624573379 | 0.7162915754 | 0.6732438832 | 0.330347542  | 0.015921      | 0.6363713636 | 0.04878       |
| togethercomputer/RedPajama-INCITE-Base-3B-v1   | RedPajama-INCITE-Base | 3              | 0.8                       | 14.4         | 0.2702787422 | 0.4018771331 | 0.64768      | 0.6471981058 | 0.3323488685 | 0.012889      | 0.6389075984 | 0.04268292683 |
| LLM360/Amber                                   | Amber                 | 7              | 1.259                     | 52.878       | 0.2683568416 | 0.409556314  | 0.7379008166 | 0.6787687451 | 0.3355637386 | 0.02805155421 | 0.6711227637 | 0.067073      |
| Salesforce/codegen-6B-nl                       | Codegen               | 6              | 0.36                      | 12.96        | 0.2593447814 | 0.4232081911 | 0.6859191396 | 0.664562     | 0.3447046127 | 0.021986      |              |               |
| Salesforce/codegen-16B-nl                      | Codegen               | 16             | 0.36                      | 34.56        | 0.3235430166 | 0.4675767918 | 0.7186815375 | 0.679558011  | 0.3394667243 | 0.026535      |              |               |
| HuggingFaceTB/SmolLM-135M                      | SmolLM                | 0.135          | 0.6                       | 0.486        | 0.2629112661 | 0.3199658703 | 0.4298944433 | 0.5240726125 | 0.392815216  | 0.010614      | 0.4996838456 | 0             |
| HuggingFaceTB/SmolLM-360M                      | SmolLM                | 0.36           | 0.6                       | 1.296        | 0.2617017733 | 0.3907849829 | 0.5403306114 | 0.563536     | 0.3804287604 | 0.018954      | 0.5297045168 | 0.012195      |
| HuggingFaceTB/SmolLM-1.7B                      | SmolLM                | 1.7            | 1                         | 10.2         | 0.301388     | 0.4931740614 | 0.6725751842 | 0.637726914  | 0.3850768935 | 0.044731      | 0.5572742616 | 0.02439       |
| cerebras/Cerebras-GPT-111M                     | Cerebras-GPT          | 0.111          | 0.00222                   | 0.001479     | 0.2551215002 | 0.20221843   | 0.2672774348 | 0.4775059195 | 0.4631298314 | 0             | 0.5049043397 | 0             |
| cerebras/Cerebras-GPT-256M                     | Cerebras-GPT          | 0.256          | 0.00512                   | 0.007864     | 0.2683072205 | 0.220137     | 0.2898824935 | 0.5248618785 | 0.4597828348 | 0             | 0.5032943609 | 0.006098      |
| cerebras/Cerebras-GPT-590M                     | Cerebras-GPT          | 0.59           | 0.0118                    | 0.041772     | 0.2597134946 | 0.2372013652 | 0.324039036  | 0.4814522494 | 0.441485     | 0.004549      | 0.5099581426 | 0.006098      |
| cerebras/Cerebras-GPT-1.3B                     | Cerebras-GPT          | 1.3            | 0.026                     | 0.2028       | 0.2659293346 | 0.2627986348 | 0.385381     | 0.5343330702 | 0.4269871365 | 0.002274      | 0.4756219239 | 0.006098      |
| cerebras/Cerebras-GPT-2.7B                     | Cerebras-GPT          | 2.7            | 0.054                     | 0.8748       | 0.2516691582 | 0.2909556314 | 0.4929296953 | 0.5414364641 | 0.413676336  | 0.004549      | 0.51198      | 0             |
| cerebras/Cerebras-GPT-6.7B                     | Cerebras-GPT          | 6.7            | 0.134                     | 5.3868       | 0.2592839463 | 0.3506825939 | 0.5936068512 | 0.5872138911 | 0.3802394599 | 0.005307      | 0.6018660709 | 0.085366      |
| cerebras/btlm-3b-8k-base                       | BTLM                  | 3              | 627                       | 11286        | 0.2716178134 | 0.4163822526 | 0.7090221071 | 0.6677190213 | 0.3591503188 | 0.041698      | 0.6514206058 | 0.1036585366  |
| h2oai/h2o-danube-1.8b-base                     | H2O-Danube            | 1.8            | 1                         | 10.8         | 0.259357     | 0.394198     | 0.6957777335 | 0.6448303078 | 0.3386425349 | 0.014405      | 0.6377463574 | 0.006098      |
| h2oai/h2o-danube2-1.8b-base                    | H2O-Danube            | 1.8            | 3                         | 32.4         | 0.4020066678 | 0.433447099  | 0.729535949  | 0.680347277  | 0.3801259785 | 0.2979529947  | 0.648862     | 0.060976      |
| h2oai/h2o-danube3-500m-base                    | H2O-Danube            | 0.5            | 4                         | 12           | 0.256073     | 0.4061433447 | 0.6050587532 | 0.6172059984 | 0.377444     | 0.1645185747  | 0.5797567282 | 0.012195      |
| h2oai/h2o-danube3-4b-base                      | H2O-Danube            | 4              | 6                         | 144          | 0.5458059832 | 0.5861774744 | 0.7984465246 | 0.7655880032 | 0.4413923587 | 0.400303      | 0.6777411282 | 0.1280487805  |
| allenai/OLMo-1B-hf                             | OLMo                  | 1              | 3                         | 18           | 0.2630669234 | 0.3455631399 | 0.6360286795 | 0.6108918706 | 0.3291625438 | 0.018954      | 0.5659539788 | 0.054878      |
| allenai/OLMo-7B-hf                             | OLMo                  | 7              | 2.5                       | 105          | 0.281323     | 0.4564846416 | 0.7730531767 | 0.6937647987 | 0.3592648258 | 0.037908      | 0.6650231931 | 0.1341463415  |
| allenai/OLMo-7B-0424-hf                        | OLMo                  | 7              | 2.05                      | 86.1         | 0.534629     | 0.4931740614 | 0.7862975503 | 0.728493     | 0.3590780612 | 0.2676269901  | 0.7026214407 | 0.1707317073  |
| TinyLlama/TinyLlama_v1.1                       | TinyLlama             | 1.1            | 3                         | 19.8         | 0.2720582997 | 0.37116      | 0.6267675762 | 0.6156274665 | 0.3508601785 | 0.015921      | 0.5962931268 | 0.006098      |

## Small Evaluations

The types of evaluations and the models evaluated are not meant to be exhaustive, there are leaderboards like the [Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) for that. What is presented is a basic set of evaluations that we used to sanity check our models during training, comparing them against how the leading (or most popular) base models of that parameter count were performing.

Running each of these evaluations on our own (even if results have previously been reported somewhere) let's us work out any kinks in the evaluation code base (e.g. multi-GPU, different architectures, etc.) on our own hardware.

## Problem Solving

The [instruct-eval](https://github.com/declare-lab/instruct-eval) harness provides a consistent way to execute a suite of 4 problem solving benchmarks, [MMLU](#mmlu), [BBH](#bbh), [HumanEval](#humaneval) and [DROP](#drop) for language and coding questions.

| Model                    | [MMLU](#mmlu)  | [BBH](#bbh)   | [HumanEval](#humaneval) | [Drop](#drop)  |
|--------------------------|-------|-------|-----------|-------|
| [btlm_3b_8k](https://huggingface.co/cerebras/btlm-3b-8k-base) | 28.01	| 30.79 |	10.98 |	17.87 |
| [llama2_7b](../architecture/llama2.md) | 45.96 | 32.04 | 14.02     | 31.57 |
| [mistral_7b_v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1) | 62.61 | 43.99 | 23.78     | 46.56 |
| [llama2_13b](../architecture/llama2.md) | 55.68 | 37.62 | 14.63     | 41.58 |
| [yi_34b](https://huggingface.co/01-ai/Yi-34B) | **76.34** | **52.15** | 17.68     | **74.65** |
| [mixtral_8x7b](../architecture/Mixtral.md) | 70.50 | 48.78 | **27.44**     | 57.74 |
| [llama2_70b](../architecture/llama2.md) | 69.12 | 50.46 | 17.68     | 62.53 |

### MMLU

[MMLU](https://arxiv.org/abs/2009.03300v3) benchmark is designed to measure world knowledge and problem-solving ability in multiple subjects.

### BBH

[BIG-Bench Hard (BBH)](https://github.com/google/BIG-bench) is a subset of 23 challenging tasks from the BIG-Bench benchmark, which focuses on tasks believed to be beyond the capabilities of current language models. It requires models to follow challenging instructions such as navigation, logical deduction, and fallacy detection.

### HumanEval

[HumanEval](https://github.com/openai/human-eval) is a problem-solving benchmark used for evaluating large language models trained on code.

### DROP

[Discrete Reasoning Over Paragraphs (DROP)](https://aclanthology.org/N19-1246/) is a math-based reading comprehension task that requires a system to perform discrete reasoning over passages extracted from Wikipedia articles.

## Commonsense Reasoning Tasks

| Model                    | Avg   | [HellaSwag](#hellaswag) | [OBQA](#obqa)  | [WinoGrande](#winogrande) | [ARC_c](#arc) | [ARC_e](#arc) | [BoolQ](#boolq) | [PIQA](#piqa)  |
|--------------------------|-----------|-------|------------|-------|-------|-------|-------|-------|
| [btlm_3b_8k](https://huggingface.co/cerebras/btlm-3b-8k-base) | 61.10 |69.66 | 40.8 |	65.82 |	37.63 |	66.92 |	69.48 |	77.42 |
| [llama2_7b](../architecture/llama2.md) | 66.71| 75.98     | 44.20 | 69.06      | 46.33 | 74.58 | 77.74 | 79.11  |
| [mistral_7b_v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1) | 71.17| 81.01     | 44.20 | 74.11      | 53.67 | 79.46 | 83.61 | 82.15  |
| [llama2_13b](../architecture/llama2.md)| 69.28| 79.38     | 45.40 | 72.45      | 49.15 | 77.53 | 80.55 | 80.52  |
| [yi_34b](https://huggingface.co/01-ai/Yi-34B) | **75.14** | 83.69     | 46.60 | **78.93**      | **61.52** | **84.26** | **88.32** | 82.64  |
| [mixtral_8x7b](../architecture/Mixtral.md)| 74.20 | **84.01** |	47.00 |	76.56 | 59.73 |	83.80 |	84.80 |	**83.51** |
| [llama2_70b](../architecture/llama2.md)| 73.61 | 83.81 |	**48.80** |	77.98 |	57.25 |	80.98 |	83.7 |	82.75 |

### HellaSwag

From the paper, [HellaSwag: Can a Machine Really Finish Your Sentence?](https://arxiv.org/abs/1905.07830).

### OBQA

[OpenBookQA (OBQA)](https://github.com/allenai/OpenBookQA) is an open book question answering dataset and benchmark.

From the paper, [Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering](https://www.semanticscholar.org/paper/24c8adb9895b581c441b97e97d33227730ebfdab).

### WinoGrande

[WinoGrande](https://github.com/allenai/winogrande) from the paper [WinoGrande: An Adversarial Winograd Schema Challenge at Scale](https://arxiv.org/abs/1907.10641).

### ARC

The [AI2's Reasoning Challenge (ARC)](https://allenai.org/data/arc) dataset is a multiple-choice question-answering dataset, containing questions from science exams from grade 3 to grade 9. The dataset is split in two partitions: Easy (ARC_e) and Challenge (ARC_c), where the latter partition contains the more difficult questions that require reasoning. Most of the questions have 4 answer choices, with <1% of all the questions having either 3 or 5 answer choices. 

### BoolQ

[BoolQ](https://github.com/google-research-datasets/boolean-questions) is a question answering dataset for yes/no questions containing 15942 examples. These questions are naturally occurring ---they are generated in unprompted and unconstrained settings.

From the paper, [BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions](https://arxiv.org/abs/1905.10044).

### PIQA

[Physical Interaction QA (PIQA)](https://github.com/ybisk/ybisk.github.io/tree/master/piqa) is a dataset for commonsense reasoning, and was created to investigate the physical knowledge of models.

PIQA is a commonsense QA benchmark for naive physics reasoning focusing on how we interact with everyday objects in everyday situations. This dataset focuses on affordances of objects, i.e., what actions each physical object affords (e.g., it is possible to use a shoe as a doorstop), and what physical interactions a group of objects afford (e.g., it is possible to place an apple on top of a book, but not the other way around). The dataset requires reasoning about both the prototypical use of objects (e.g., shoes are used for walking) and non-prototypical but practically plausible use of objects (e.g., shoes can be used as a doorstop). The dataset includes 20,000 QA pairs that are either multiple-choice or true/false questions.

From the paper, [PIQA: Reasoning about Physical Commonsense in Natural Language](https://arxiv.org/abs/1911.11641v1).