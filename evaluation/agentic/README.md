# Agentic and Tool Use

Evaluations that assess the ability to interact with external tools and environments, perform actions based on goals, and exhibit autonomous behavior.  

For more information on workflows and agents, see the [agentic section](../../applications/agents/).

- [BFCL (Berkeley Function-Calling Leaderboard)](bfcl.md) - comprises question-function-answer pairs, where a user's question is paired with a set of available functions and the expected answer.
- [TAU-Bench (Ï„ -bench)](taubench.md) - tests if an agent can reliably engage in a dynamic, multi-turn conversation with a user to figure out what needs to be done. The benchmark includes 10-15 tools across 50-115 different tasks for a retail and airline domain.
- [Nexus (Nexus Function Calling Benchmark)](nexus.md) - functional calling/tool use benchmark with 9 tasks covering a spectrum, from cybersecurity and climate APIs to recommendation systems, along with some pure Python functions.
- [PaperBench](paperbench.md) - evaluates the ability of AI agents to replicate replicate 20 ICML 2024 Spotlight and Oral papers from scratch, including understanding paper contributions, developing a codebase, and successfully executing experiments.